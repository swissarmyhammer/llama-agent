Embedding needs to have the same --debug llama logging supression logic as generation.

Ôê≤ cargo run --package llama-cli embed \
  --model Qwen/Qwen3-Embedding-0.6B-GGUF \
  --input texts.txt \
  --output embeddings.parquet \
  --batch-size 32 \
  --normalize
   Compiling llama-cli v0.1.0 (/Users/wballard/github/llama-agent/llama-cli)
    Finished `dev` profile [optimized + debuginfo] target(s) in 1.01s
     Running `target/debug/llama-cli embed --model Qwen/Qwen3-Embedding-0.6B-GGUF --input texts.txt --output embeddings.parquet --batch-size 32 --normalize`
Loading model: Qwen/Qwen3-Embedding-0.6B-GGUF
llama_model_load_from_file_impl: using device Metal (Apple M2 Ultra) - 147455 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 310 tensors from /Users/wballard/Library/Caches/llama-loader/models/bfb7e3e0353e41b4d033ff88f35852a955f9b76b2e831a5e35b07106d44009b3_Qwen3-Embedding-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 Embedding 0.6b
llama_model_loader: - kv   3:                           general.basename str              = qwen3-embedding
llama_model_loader: - kv   4:                         general.size_label str              = 0.6B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                   general.base_model.count u32              = 1
llama_model_loader: - kv   7:                  general.base_model.0.name str              = Qwen3 0.6B Base
llama_model_loader: - kv   8:          general.base_model.0.organization str              = Qwen


## Proposed Solution

After investigating the codebase, I found that the generation command already has debug logging suppression logic in `/Users/wballard/github/llama-agent/llama-agent/src/model.rs` (lines 55-66). The `ModelManager::new` function uses:

1. `set_logging_suppression(true)` when debug is false to suppress verbose llama.cpp output
2. `set_logging_suppression(false)` when debug is true to allow llama.cpp logs

However, the embedding functionality in `/Users/wballard/github/llama-agent/llama-embedding/src/model.rs` doesn't have this same logic. The `EmbeddingModel::new` function passes the debug flag to the ModelConfig but doesn't actually call the logging suppression functions.

### Implementation Steps:
1. Add the same logging suppression logic to `EmbeddingModel::new` that exists in `ModelManager::new`
2. Import the necessary FFI bindings and callback functions
3. Apply `set_logging_suppression()` based on the debug flag in the config

This will make the embedding command behavior consistent with the generation command regarding llama.cpp verbose output suppression.